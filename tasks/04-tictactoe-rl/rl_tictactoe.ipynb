{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra]"
      ],
      "metadata": {
        "id": "4SvzOyq-bSaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YVdRKnVaXR6s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "class TicTacToeEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Класс TicTacToeEnv представляет собой среду для игры в крестики-нолики.\n",
        "  Библиотека gym предоставляет интерфейс для создания и управлениz средами обучения.\n",
        "  \"\"\"\n",
        "\n",
        "  metadata = {'render.moves': ['human']}\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Инициализация среды, определение пространства действий и состояний.\n",
        "    \"\"\"\n",
        "    super(TicTacToeEnv, self).__init__()\n",
        "    self.action_space = spaces.Discrete(9)\n",
        "    self.observation_space = spaces.Box(low=0, high=2, shape=(3, 3), dtype=np.int)\n",
        "    self.state = np.zeros((3, 3), dtype=np.int)\n",
        "    self.current_player = 1\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Выполнение хода в игре. Принимает действие, обновляет состояние игры,\n",
        "    возвращает новое состояние, награду, флаг завершения игры.\n",
        "    \"\"\"\n",
        "    if self.is_valid_action(action):\n",
        "      row, col = divmod(action, 3)\n",
        "      self.state[row, col] = self.current_player\n",
        "      winner = self.get_winner()\n",
        "      done = winner is not None or np.all(self.state != 0)\n",
        "      reward = 1 if winner == self.current_player else 0\n",
        "      self.switch_player()\n",
        "      return self.state, reward, done, {}\n",
        "    else:\n",
        "      return self.state, -1, True, {}\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Сброс состояния среды к начальному\n",
        "    \"\"\"\n",
        "    self.state = np.zeros((3, 3), dtype=np.int)\n",
        "    self.current_player = 1\n",
        "    return self.state\n",
        "\n",
        "  def render(self, mode='human', close=False):\n",
        "    \"\"\"\n",
        "    Отображение текущего состояния игрового поля.\n",
        "    \"\"\"\n",
        "    if close:\n",
        "      return\n",
        "    for row in self.state:\n",
        "      print(\" \".join('XO'[i-1] if i > 0 else '.' for i in row))\n",
        "    print(\"\")\n",
        "\n",
        "  def close(self):\n",
        "    \"\"\"\n",
        "    Закрытие среды.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def switch_player(self):\n",
        "    \"\"\"\n",
        "    Смена текущего игрока.\n",
        "    \"\"\"\n",
        "    self.current_player = 3 - self.current_player\n",
        "\n",
        "  def is_valid_action(self, action):\n",
        "    \"\"\"\n",
        "    Проверка, является ли действие допустимым (можно ли сделать ход в выбранную ячейку).\n",
        "    \"\"\"\n",
        "    row, col = divmod(action, 3)\n",
        "    return self.state[row, col] == 0\n",
        "\n",
        "  def is_game_over(self):\n",
        "    \"\"\"\n",
        "    Проверка, закончилась ли игра (есть победитель или все ячейки заполнены).\n",
        "    \"\"\"\n",
        "    return self.get_winner() is not None or np.all(self.state != 0)\n",
        "\n",
        "  def get_winner(self):\n",
        "    \"\"\"\n",
        "    Проверка строк, столбцов и диагоналей на наличие победителя.\n",
        "    \"\"\"\n",
        "    for player in [1, 2]:\n",
        "      if np.any(np.all(self.state == player, axis=0)) or \\\n",
        "        np.any(np.all(self.state == player, axis=1)) or \\\n",
        "        np.all(np.diag(self.state) == player) or \\\n",
        "        np.all(np.diag(np.fliplr(self.state)) == player):\n",
        "        return player\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "vec_env = make_vec_env(lambda: TicTacToeEnv(), n_envs=4)\n",
        "\n",
        "# Используем моедль PPO\n",
        "model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
        "\n",
        "# Оценим начальную производительность агента (до обучения)\n",
        "mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=10)\n",
        "print(f\"Средняя награда до обучения: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "# Обучим модель\n",
        "model.learn(total_timesteps=125000)\n",
        "\n",
        "# Оценим производительность агента после обучения\n",
        "mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=10)\n",
        "print(f\"Средняя награда после обучения: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "model.save(\"ppo_tictactoe\")"
      ],
      "metadata": {
        "id": "o7WwCO3va0iW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44193f6-181f-4ec4-fa4f-f154dc206467"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-498ce4cfacea>:19: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self.observation_space = spaces.Box(low=0, high=2, shape=(3, 3), dtype=np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Средняя награда до обучения: -1.00 +/- 0.00\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.61     |\n",
            "|    ep_rew_mean     | -0.92    |\n",
            "| time/              |          |\n",
            "|    fps             | 2931     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.7         |\n",
            "|    ep_rew_mean          | -0.86       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1286        |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020358715 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.19       |\n",
            "|    explained_variance   | -0.417      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0229      |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0251     |\n",
            "|    value_loss           | 0.203       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 5.27       |\n",
            "|    ep_rew_mean          | -0.66      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1068       |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 22         |\n",
            "|    total_timesteps      | 24576      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02202722 |\n",
            "|    clip_fraction        | 0.175      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.14      |\n",
            "|    explained_variance   | -0.0103    |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0895     |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | -0.0251    |\n",
            "|    value_loss           | 0.27       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 6.11        |\n",
            "|    ep_rew_mean          | -0.36       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1040        |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 31          |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022809051 |\n",
            "|    clip_fraction        | 0.177       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.07       |\n",
            "|    explained_variance   | -0.0057     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.197       |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0308     |\n",
            "|    value_loss           | 0.468       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 6.52        |\n",
            "|    ep_rew_mean          | -0.26       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1008        |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 40          |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020489521 |\n",
            "|    clip_fraction        | 0.159       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.97       |\n",
            "|    explained_variance   | 0.0101      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.286       |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0309     |\n",
            "|    value_loss           | 0.588       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 6.91        |\n",
            "|    ep_rew_mean          | 0.23        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1001        |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 49          |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016593318 |\n",
            "|    clip_fraction        | 0.133       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.87       |\n",
            "|    explained_variance   | -0.00333    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.303       |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0296     |\n",
            "|    value_loss           | 0.684       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 7.21        |\n",
            "|    ep_rew_mean          | 0.3         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 986         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 58          |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013578432 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.77       |\n",
            "|    explained_variance   | -0.00834    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.283       |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.026      |\n",
            "|    value_loss           | 0.651       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 7.17        |\n",
            "|    ep_rew_mean          | 0.43        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 962         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 68          |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012142847 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.7        |\n",
            "|    explained_variance   | 0.00761     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.225       |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.024      |\n",
            "|    value_loss           | 0.571       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 7.17        |\n",
            "|    ep_rew_mean          | 0.49        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 969         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 76          |\n",
            "|    total_timesteps      | 73728       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010297795 |\n",
            "|    clip_fraction        | 0.1         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.64       |\n",
            "|    explained_variance   | -0.0103     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.185       |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0201     |\n",
            "|    value_loss           | 0.452       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 7.42        |\n",
            "|    ep_rew_mean          | 0.72        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 966         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 84          |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009644109 |\n",
            "|    clip_fraction        | 0.0956      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.59       |\n",
            "|    explained_variance   | -0.00239    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.136       |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0171     |\n",
            "|    value_loss           | 0.38        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 7.17        |\n",
            "|    ep_rew_mean          | 0.72        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 963         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 93          |\n",
            "|    total_timesteps      | 90112       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010959381 |\n",
            "|    clip_fraction        | 0.125       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.55       |\n",
            "|    explained_variance   | 0.0113      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.133       |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0181     |\n",
            "|    value_loss           | 0.301       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 7.08        |\n",
            "|    ep_rew_mean          | 0.79        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 969         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 101         |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010838473 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.5        |\n",
            "|    explained_variance   | 0.00838     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0543      |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0155     |\n",
            "|    value_loss           | 0.232       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 7.06        |\n",
            "|    ep_rew_mean          | 0.84        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 967         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 110         |\n",
            "|    total_timesteps      | 106496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011771984 |\n",
            "|    clip_fraction        | 0.135       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.46       |\n",
            "|    explained_variance   | -0.0149     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.12        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0153     |\n",
            "|    value_loss           | 0.206       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 7.3         |\n",
            "|    ep_rew_mean          | 0.91        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 976         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 117         |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010558313 |\n",
            "|    clip_fraction        | 0.15        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.43       |\n",
            "|    explained_variance   | 0.0119      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0236      |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0145     |\n",
            "|    value_loss           | 0.152       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 7.28        |\n",
            "|    ep_rew_mean          | 0.75        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 973         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 126         |\n",
            "|    total_timesteps      | 122880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011162851 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.38       |\n",
            "|    explained_variance   | 0.0366      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0617      |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0136     |\n",
            "|    value_loss           | 0.131       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 7.07        |\n",
            "|    ep_rew_mean          | 0.89        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 981         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 133         |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010313085 |\n",
            "|    clip_fraction        | 0.136       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.36       |\n",
            "|    explained_variance   | 0.0592      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0277      |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0139     |\n",
            "|    value_loss           | 0.131       |\n",
            "-----------------------------------------\n",
            "Средняя награда после обучения: 1.00 +/- 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Случайный агент\n",
        "def random_agent(state):\n",
        "  \"\"\"\n",
        "  Агент, который выбирает случайный допустимый ход.\n",
        "  :param state: текущее состояние игрового поля\n",
        "  :return: индекс действия\n",
        "  \"\"\"\n",
        "  available_actions = [i for i in range(9) if state[i // 3, i % 3] == 0]\n",
        "  return np.random.choice(available_actions) if available_actions else None\n",
        "\n",
        "# Игра со случайным агентом\n",
        "def play_games(model, env, num_games):\n",
        "    wins = 0\n",
        "    losses = 0\n",
        "    draws = 0\n",
        "\n",
        "    for _ in range(num_games):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs)\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            if done:\n",
        "                if reward == 1:\n",
        "                    wins += 1\n",
        "                elif reward == -1:\n",
        "                    losses += 1\n",
        "                else:\n",
        "                    draws += 1\n",
        "            else:\n",
        "                # Если игра не закончилась, случайный агент делает ход\n",
        "                action = random_agent(obs)\n",
        "                obs, reward, done, info = env.step(action)\n",
        "\n",
        "    print(f\"Агент выиграл {wins} игр, проиграл {losses} игр, сыграл вничью {draws} игр.\")\n",
        "\n",
        "play_games(model, TicTacToeEnv(), num_games=500)\n"
      ],
      "metadata": {
        "id": "f8mjQm0XczAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a97fee8-be54-4338-bd7f-ca8bd1977ba6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-498ce4cfacea>:19: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self.observation_space = spaces.Box(low=0, high=2, shape=(3, 3), dtype=np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Агент выиграл 324 игр, проиграл 4 игр, сыграл вничью 41 игр.\n"
          ]
        }
      ]
    }
  ]
}